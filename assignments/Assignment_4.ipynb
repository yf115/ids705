{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 4 - Unsupervised Learning and Neural Networks"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *YOUR FULL NAME HERE*\n",
        "Netid: Your netid here"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions for all assignments can be found [here](https://github.com/kylebradbury/ids705/blob/master/assignments/_Assignment%20Instructions.ipynb), which is also linked to from the [course syllabus](https://kylebradbury.github.io/ids705/index.html)."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Learning objectives\n",
        "Through completing this assignment you will be able to...\n",
        "1. Apply clustering techniques to a variety of datasets with diverse distributional properties, gaining an understanding of their strengths and weaknesses and how to tune model parameters.\n",
        "2. Apply PCA and t-SNE for performing dimensionality reduction and data visualization\n",
        "3. Understand how PCA represents data in lower dimensions and understand the concept of data compression.\n",
        "4. Build, tune the parameters of, and apply feedforward neural networks to data\n",
        "5. Develop a detailed understanding of the math and practical implementation considerations of neural networks, one of the most widely used machine learning tools."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1\n",
        "\n",
        "### [35 points] Clustering\n",
        "\n",
        "Clustering can be used to reveal structure between samples of data and assign group membership to similar groups of samples. This exercise will provide you with experience building a basic clustering algorithm to provide insight into the structure of these techniques, then compare a number of clustering techniques on a distinctive datasets to experience the pros and cons of these approaches.\n",
        "\n",
        "**(a)** Implement your own k-means algorithm. For a measure of dissimilarity use the sum of squared error of the Euclidean distance from each point to the cluster mean. Initialize your means by selecting a set of $k$ points at random from your dataset and using those values as the initial means. You may need to run your algorithm multiple times with different initializations (picking the clustering with the lower dissimilarity measure) to get the best results. You may use the template below to assist you in your implementation. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def kmeans(X, k, max_steps=, convergence_threshold=):\n",
        "    '''kmeans\n",
        "    \n",
        "        Input:\n",
        "            X: matrix of input data where each row represents a sample\n",
        "            k: number of means to use\n",
        "            max_steps: maximum number of iterations to run the algorithm\n",
        "            convergence_threshold: if the means change less than this\n",
        "                                   value in an iteration, declare convergence\n",
        "        Output:\n",
        "            means: a matrix listing the k means\n",
        "            cluster_assignment: a list of the cluster assignments for each samples\n",
        "            dissimilarity: sum of squared error of the Euclidean distance from each point to the cluster mean\n",
        "    '''"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(b)** Demo your algorithm. Create some data to cluster by using the `blobs` module from `scikit-learn` to construct two datasets: one with 2 cluster centers and the other with 5. Set the `random_state` keyword parameter to 0 in each to ensure the datasets are consistent with the rest of the class and generate 5,000 samples of each dataset. For each dataset rerun your k-means algorithm for values of $k$ ranging from 1 to 10 and for each plot the \"elbow curve\" where you plot dissimilarity in each case. For your two datasets, where is the elbow in the curve and why? Plot the data and your $k$-means for the optimal value of $k$ that you determined from the elbow curve.\n",
        "\n",
        "**(c)** Ensure your understanding of how clustering methods work. Briefly explain in 1-2 sentences each (at a very high level) how the following clustering techniques work and what distinguishes them from other clustering methods: (1) k-means, (2) agglomerative clustering, (3) Gaussian mixture models, (4) DBSCAN, and (5) spectral clustering.\n",
        "\n",
        "**(d)** Apply clustering algorithms to diverse datasets. For each of the clustering algorithms in (c) run each of them on the five datasets below. Tune the parameters in each model to achieve better performance for each dataset. Plot the final result as a 4-by-5 subplot showing the resulting clustering of each method on each dataset. Which method works best or worst on each dataset and why? (This can be 1-2 sentences for each dataset). The datasets are:\n",
        "- Aggregation.txt\n",
        "- Compound.txt\n",
        "- D31.txt\n",
        "- jain.txt\n",
        "\n",
        "Each file has three columns: the first two are $x_1$ and $x_2$, then the third is a suggested cluster label (ignore this third column - do NOT include this in your analysis). *The data are from https://cs.joensuu.fi/sipu/datasets/*.\n",
        "\n",
        "*Note: for k-means, use the `scikit-learn` module rather than your own*"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ANSWER**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2\n",
        "\n",
        "### [20 points] Dimensionality reduction and visualization of digits with PCA and t-SNE\n",
        "\n",
        "**(a)** Reduce the dimensionality of the data with PCA for data visualization. Load the `scikit-learn` digits dataset. Apply PCA and reduce the data (with the associated cluster labels 0-9) into a 2-dimensional space. Plot the data with labels in this two dimensional space (labels can be colors, shapes, or using the actual numbers to represent the data - definitely include a legend in your plot).\n",
        "\n",
        "**(b)** Create a plot showing the cumulative fraction of variance explained as you incorporate from $1$ through all $D$ principal components of the data (where $D$ is the dimensionality of the data). What fraction of variance in the data is UNEXPLAINED by the first two principal components of the data? Briefly comment on how this may impact how well-clustered the data are. *You can use the `explained_variance_` attribute of the PCA module in `scikit-learn` to assist with this question*\n",
        "\n",
        "**(c)** Reduce the dimensionality of the data with t-SNE for data visualization. T-distributed stochastic neighborhood embedding (t-SNE) is a nonlinear dimensionality reduction technique that is particularly adept at embedding the data into lower 2 or 3 dimensional spaces. Apply t-SNE to the digits dataset and plot it in 2-dimensions (with associated cluster labels 0-9). You may need to adjust the parameters to get acceptable performance. You can read more about how to use t-SNE effectively [here](https://distill.pub/2016/misread-tsne/).\n",
        "\n",
        "**(d)** Compare/contrast the performance of these two techniques. Which seemed to cluster the data best and why? *Note: You typically will not have labels available in most problems.*"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ANSWER**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3\n",
        "\n",
        "### [45 points] Build and test your own Neural Network for classification\n",
        "\n",
        "There is no better way to understand how one of the core techniques of modern machine learning works than to build a simple version of it yourself. In this exercise you will construct and apply your own neural network classifier. You may use numpy if you wish but no other libraries.\n",
        "\n",
        "**(a)** Create a neural network class that follows the `scikit-learn` classifier convention by implementing `fit`, `predict`, and `predict_proba` methods. Your `fit` method should run backpropagation on your training data using stochastic gradient descent. Assume the activation function is a sigmoid. Choose your model architecture to have two input nodes, two hidden layers with five nodes each, and one output node.\n",
        "\n",
        "To guide you in the right direction with this problem, please find a skeleton of a neural network class below. You absolutely MAY use additional methods beyond those suggested in this template, but I see these methods as the minimum required to implement the model cleanly.\n",
        "\n",
        "One of the greatest challenges of this implementations is that there are many parts and a bug could be present in any of them. I would strongly encourage you to create unit tests for most modules. Without doing this will make your code extremely difficult to bug. You can create simple examples to feed through the network to validate it is correctly computing activations and node values. Also, if you manually set the weights of the model, you can even calculate backpropagation by hand for some simple examples (admittedly, that unit test would be challenging, but a unit test is possible). You can also verify the performance of your overall neural network by comparing it against the `scikit-learn` implementation and using the same architecture and parameters as your model."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(b)** Apply your neural network. Create a training and validation dataset using `sklearn.datasets.make_moons(N, noise=0.20)`, where $N_{train} = 500$ and $N_{test} = 100$. Train and test your model on this dataset plotting your learning curves (training and validation error for each epoch of stochastic gradient descent, where an epoch represents having trained on each of the training samples one time). Adjust the learning rate and number of training epochs for your model to improve performance as needed. In two subplots, plot the training data on one subplot, and the validation data on the other subplot. On each plot, also plot the decision boundary from your neural network trained on the training data. Report your performance on the test data with an ROC curve and compare against the `scikit-learn` `MLPClassifier` trained with the same parameters.\n",
        "\n",
        "**(c)** Suggest two ways in which you neural network implementation could be improved."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "class myNeuralNetwork(object):\n",
        "    \n",
        "    def __init__(self, n_in, n_layer1, n_layer2, n_out, learning_rate=):\n",
        "        '''__init__\n",
        "        Class constructor: Initialize the parameters of the network including\n",
        "        the learning rate, layer sizes, and each of the parameters\n",
        "        of the model (weights, placeholders for activations, inputs, \n",
        "        deltas for gradients, and weight gradients). This method\n",
        "        should also initialize the weights of your model randomly\n",
        "            Input:\n",
        "                n_in:          number of inputs\n",
        "                n_layer1:      number of nodes in layer 1\n",
        "                n_layer2:      number of nodes in layer 2\n",
        "                n_out:         number of output nodes\n",
        "                learning_rate: learning rate for gradient descent\n",
        "            Output:\n",
        "                none\n",
        "        '''\n",
        "            \n",
        "    def forward_propagation(self, x):\n",
        "        '''forward_propagation\n",
        "        Takes a vector of your input data (one sample) and feeds\n",
        "        it forward through the neural network, calculating activations and\n",
        "        layer node values along the way.\n",
        "            Input:\n",
        "                x: a vector of data represening 1 sample [n_in x 1]\n",
        "            Output:\n",
        "                y_hat: a vector (or scaler of predictions) [n_out x 1]\n",
        "                (typically n_out will be 1 for binary classification)\n",
        "        '''\n",
        "    \n",
        "    def compute_loss(self, X, y):\n",
        "        '''compute_loss\n",
        "        Computes the current loss/cost function of the neural network\n",
        "        based on the weights and the data input into this function.\n",
        "        To do so, it runs the X data through the network to generate\n",
        "        predictions, then compares it to the target variable y using\n",
        "        the cost/loss function\n",
        "            Input:\n",
        "                X: A matrix of N samples of data [N x n_in]\n",
        "                y: Target variable [N x 1]\n",
        "            Output:\n",
        "                loss: a scalar measure of loss/cost\n",
        "        '''\n",
        "    \n",
        "    def backpropagate(self, x, y):\n",
        "        '''backpropagate\n",
        "        Backpropagate the error from one sample determining the gradients\n",
        "        with respect to each of the weights in the network. The steps for\n",
        "        this algorithm are:\n",
        "            1. Run a forward pass of the model to get the activations \n",
        "               Corresponding to x and get the loss functionof the model \n",
        "               predictions compared to the target variable y\n",
        "            2. Compute the deltas (see lecture notes) and values of the\n",
        "               gradient with respect to each weight in each layer moving\n",
        "               backwards through the network\n",
        "    \n",
        "            Input:\n",
        "                x: A vector of 1 samples of data [n_in x 1]\n",
        "                y: Target variable [scalar]\n",
        "            Output:\n",
        "                loss: a scalar measure of th loss/cost associated with x,y\n",
        "                      and the current model weights\n",
        "        '''\n",
        "        \n",
        "    def stochastic_gradient_descent_step(self):\n",
        "        '''stochastic_gradient_descent_step\n",
        "        Using the gradient values computer by backpropagate, update each\n",
        "        weight value of the model according to the familiar stochastic\n",
        "        gradient descent update equation.\n",
        "        \n",
        "        Input: none\n",
        "        Output: none\n",
        "        '''\n",
        "    \n",
        "    def fit(self, X, y, max_epochs=, learning_rate=, get_validation_loss=):\n",
        "        '''fit\n",
        "            Input:\n",
        "                X: A matrix of N samples of data [N x n_in]\n",
        "                y: Target variable [N x 1]\n",
        "            Output:\n",
        "                training_loss:   Vector of training loss values at the end of each epoch\n",
        "                validation_loss: Vector of validation loss values at the end of each epoch\n",
        "                                 [optional output if get_validation_loss==True]\n",
        "        '''\n",
        "            \n",
        "    def predict_proba(self, X):\n",
        "        '''predict_proba\n",
        "        Compute the output of the neural network for each sample in X, with the last layer's\n",
        "        sigmoid activation providing an estimate of the target output between 0 and 1\n",
        "            Input:\n",
        "                X: A matrix of N samples of data [N x n_in]\n",
        "            Output:\n",
        "                y_hat: A vector of class predictions between 0 and 1 [N x 1]\n",
        "        '''\n",
        "    \n",
        "    def predict(self, X, decision_thresh=):\n",
        "        '''predict\n",
        "        Compute the output of the neural network prediction for \n",
        "        each sample in X, with the last layer's sigmoid activation \n",
        "        providing an estimate of the target output between 0 and 1, \n",
        "        then thresholding that prediction based on decision_thresh\n",
        "        to produce a binary class prediction\n",
        "            Input:\n",
        "                X: A matrix of N samples of data [N x n_in]\n",
        "                decision_threshold: threshold for the class confidence score\n",
        "                                    of predict_proba for binarizing the output\n",
        "            Output:\n",
        "                y_hat: A vector of class predictions of either 0 or 1 [N x 1]\n",
        "        '''\n",
        "    \n",
        "    def sigmoid(self, X):\n",
        "        '''sigmoid\n",
        "        Compute the sigmoid function for each value in matrix X\n",
        "            Input:\n",
        "                X: A matrix of any size [m x n]\n",
        "            Output:\n",
        "                X_sigmoid: A matrix [m x n] where each entry corresponds to the\n",
        "                           entry of X after applying the sigmoid function\n",
        "        '''\n",
        "    \n",
        "    def sigmoid_derivative(self, X):\n",
        "        '''sigmoid_derivative\n",
        "        Compute the sigmoid derivative function for each value in matrix X\n",
        "            Input:\n",
        "                X: A matrix of any size [m x n]\n",
        "            Output:\n",
        "                X_sigmoid: A matrix [m x n] where each entry corresponds to the\n",
        "                           entry of X after applying the sigmoid derivative function\n",
        "        '''\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ANSWER**"
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.3",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "position": {
        "height": "644px",
        "left": "1548px",
        "right": "20px",
        "top": "120px",
        "width": "367px"
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": true
    },
    "nteract": {
      "version": "0.22.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}